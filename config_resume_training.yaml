checkpoints:
    checkpoint_interval: 1000
    checkpoints_path: checkpoints
    checkpoints_path_is_shared_file_system: false
    load_lr_scheduler: false
    load_optimizer: false
    resume_checkpoint_path: checkpoints/llama-3.2-3B-nanotron
    save_final_state: true
    save_initial_state: false
data_stages:
    - data:
          dataset: null
          num_loading_workers: 1
          seed: 42
      name: Stable Training Stage
      start_training_step: 1
general:
    benchmark_csv_path: null
    consumed_train_samples: null
    ignore_sanity_checks: false
    project: resume_training
    run: resume_training_20250411_155705_local
    seed: 42
    step: null
lighteval: null
logging:
    iteration_step_info_interval: 1
    log_level: info
    log_level_replica: info
metrics_logging: null
model:
    ddp_bucket_cap_mb: 25
    dtype: bfloat16
    init_method:
        std: 0.025
    make_vocab_size_divisible_by: 1
    model_config:
        _attn_implementation: flash_attention_2
        _fused_rms_norm: true
        _fused_rotary_emb: true
        _use_doc_masking: true
        _use_qkv_packed: true
        attention_bias: false
        bos_token_id: 128000
        eos_token_id: 128001
        flex_attention_mask: null
        hidden_act: silu
        hidden_size: 3072
        initializer_range: 0.02
        intermediate_size: 8192
        is_qwen2_config: true
        max_position_embeddings: 131072
        moe_config: null
        no_rope_layer: null
        num_attention_heads: 24
        num_hidden_layers: 28
        num_key_value_heads: 8
        pad_token_id: null
        pretraining_tp: 1
        rms_norm_eps: 1.0e-05
        rope_interleaved: false
        rope_scaling:
            factor: 32.0
            high_freq_factor: 4.0
            low_freq_factor: 1.0
            original_max_position_embeddings: 8192
            rope_type: llama3
        rope_theta: 500000.0
        sliding_window_size: null
        tie_word_embeddings: true
        use_cache: false
        vocab_size: 128256
        z_loss_coefficient: 0.0001
        z_loss_enabled: false
optimizer:
    accumulate_grad_in_fp32: false
    clip_grad: 1.0
    learning_rate_scheduler:
        learning_rate: 0.0003
        lr_decay_starting_step: null
        lr_decay_steps: 13
        lr_decay_style: cosine
        lr_warmup_steps: 2
        lr_warmup_style: linear
        min_decay_lr: 1.0e-05
    optimizer_factory:
        adam_beta1: 0.9
        adam_beta2: 0.95
        adam_eps: 1.0e-08
        name: adamW
        torch_adam_is_fused: true
    weight_decay: 0.01
    weight_decay_exclude_named_params: []
    zero_stage: 0
parallelism:
    context_parallel_size: 1
    dp: 1
    expert_parallel_size: 1
    moe_layer_recompute: false
    pp: 1
    pp_engine: 1f1b
    recompute_layer: true
    tp: 2
    tp_linear_async_communication: true
    tp_mode: REDUCE_SCATTER
    tp_recompute_allgather: true
profiler: null
s3_upload: null
tokenizer:
    tokenizer_max_length: null
    tokenizer_name_or_path: /scratch/oa2451/prog-assignment-1/climate-optimized-Llama-3B/llama-3.2-3B
    tokenizer_revision: null
tokens:
    batch_accumulation_per_replica: 1
    limit_test_batches: 0
    limit_val_batches: 0
    micro_batch_size: 1
    sequence_length: 256
    train_steps: 426
    val_check_interval: -1

